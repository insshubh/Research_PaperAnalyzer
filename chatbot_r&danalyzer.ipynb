{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insshubh/Research_PaperAnalyzer/blob/main/chatbot_r%26danalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tJbEd5oJqkqi"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import string\n",
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "8HyL-BCkVc6x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QliCQqlRsqzG"
      },
      "source": [
        "Reading the Corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "T_vt15LSsy7O"
      },
      "outputs": [],
      "source": [
        "f = open('/content/drive/MyDrive/test_analyzer.csv','r',errors = 'ignore')\n",
        "raw = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3gB9khRuFYS",
        "outputId": "da000315-0fd6-4ebe-b1ef-3557f67f55a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "raw = raw.lower() # converting entire text to lower case\n",
        "nltk.download('punkt') # tokenizer which train on like twitter dataset\n",
        "nltk.download('wordnet')# dictionary\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mV833Jd6waPa"
      },
      "outputs": [],
      "source": [
        "sentence_tokens = nltk.sent_tokenize(raw)\n",
        "word_tokens = nltk.word_tokenize(raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvWZZfgv9VR"
      },
      "source": [
        "After Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vKmq6s1wASf",
        "outputId": "caac623d-aee3-4463-a6ff-5fa4fc1cbe86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"i've put this cheatsheet together to help you become knowledgeable and effective with react hooks as quickly as possible.\",\n",
              " 'plus, this tutorial is also an interactive video guide that will show you practical examples of how to use each hook in 30 seconds or less.',\n",
              " 'each example builds off of the previous one and it includes tons of patterns and best practices that will help you build apps with react hooks for years to come.',\n",
              " 'want your own copy?\\u202c\\nclick here to download the cheatsheet in pdf format (it takes 5 seconds).',\n",
              " \"here are 3 quick wins you get when you grab the downloadable version:\\n\\nyou'll get tons of copyable code snippets for easy reuse in your own projects.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "sentence_tokens[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx9hd-Rhwogj",
        "outputId": "88e0bea7-6606-4913-da44-4d2bcc80acfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', \"'ve\", 'put', 'this', 'cheatsheet']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "word_tokens[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHcHxsniwxI9"
      },
      "source": [
        "Text-Preprocessing steps\n",
        "It will remove all coma and unnecessary words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gvh515xkw2f1"
      },
      "outputs": [],
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punc_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0caoZrSXyBHf"
      },
      "source": [
        "Greeting function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9b_KUGksyHh4"
      },
      "outputs": [],
      "source": [
        "greet_inputs=('hello','hey','how are you?')\n",
        "greet_responses = ('hi','hey','Bonjoir','There There!')\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in greet_inputs:\n",
        "      return random.choice(greet_responses)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxEf6tNby49V"
      },
      "source": [
        "Response Generation by Bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VNeQb9aHy7y8"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "custom_stop_words = set(['ha', 'le', 'u', 'wa'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "feeYGy801mKP"
      },
      "outputs": [],
      "source": [
        "def response(user_response):\n",
        "  robo1_response=''\n",
        "  Tfidvec =TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')\n",
        "  tfidf = Tfidvec.fit_transform(sentence_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if req_tfidf==0:\n",
        "    robo1_response = robo1_response + \"I am Sorry. Unable to understand You!\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response = robo1_response + sentence_tokens[idx]\n",
        "    return robo1_response\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyIFCOFs3n3o"
      },
      "source": [
        "Defining the ChatFLow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Aa8JVtmI3qk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26bc87f6-6f1e-4b41-e55f-152c57a02389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello ! I am the Auto Learning Bot. Start typing your text after Greeting to talk to me. For ending type bye!\n",
            "hello\n",
            "Bot :Bonjoir\n",
            "what is usestate\n",
            "Bot :"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there's a ton of great stuff to cover, so let's get started:\n",
            "\n",
            "table of contents:\n",
            "usestate hook\n",
            "useeffect hook\n",
            "useref hook\n",
            "usecallback hook\n",
            "usememo hook\n",
            "usecontext hook\n",
            "usereducer hook\n",
            "1. usestate hook\n",
            "usestate to create state variables\n",
            "the usestate hook allows us to create state variables in a react function component.\n",
            "can u tell me about React\n",
            "Bot :state allows us to access and update certain values in our components over time\n",
            "when we create a state variable, we must provide it a default value (which can be any data type).\n",
            "usefilter\n",
            "Bot :I am Sorry. Unable to understand You!\n",
            "ha\n",
            "Bot :I am Sorry. Unable to understand You!\n",
            "stop\n",
            "Bot :I am Sorry. Unable to understand You!\n",
            "exit\n",
            "Bot : Revoir!\n"
          ]
        }
      ],
      "source": [
        "flag = True\n",
        "print('Hello ! I am the Auto Learning Bot. Start typing your text after Greeting to talk to me. For ending type bye!')\n",
        "exit = ('bye','tata','exit','roko')\n",
        "while flag == True:\n",
        "  user_response = input().lower()\n",
        "  if user_response not in exit:\n",
        "    if user_response == 'thank you' or user_response == 'thanks':\n",
        "      flag =False\n",
        "      print(\"Bot : You are Welcome \")\n",
        "    else:\n",
        "      if greet(user_response) != None:\n",
        "        print(\"Bot :\"+ greet(user_response))\n",
        "      else:\n",
        "          sentence_tokens.append(user_response)\n",
        "          word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "          final_words = list(set(word_tokens))\n",
        "          print(\"Bot :\", end =\"\")\n",
        "          print(response(user_response))\n",
        "          sentence_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag = False\n",
        "    print(\"Bot : Revoir!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1f6SK0BWgVBo3MXNirdGE2Faj0pChQb9F",
      "authorship_tag": "ABX9TyO25sZJOjfyAA605x+KsBS3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}